{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Quote Scraping Code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The challenge in this portion of the project is to produce data that is properly organized and presented in a manner that faciltates the storage in a database.  While one could write simple 'find_all' code to scrape data from the site, it would not A. be automated B. be organized and C. nor structured appropriately for storage.  \n",
    "### The approach taken here was to ensure that the code provided for ensuring that all of these points were considered in designing the code.  First the code needed to provide for a one touch crawl of the whole site that is both complete and rapid/efficient.  The second requirement was to maintaion the orgnaization of the data.  Simple scraping commands will return unorganized data that retains none of its value once collected as the datas relationships and organization are integral to its value.  The third achieved goal for the code was to present the data prepared for database storage.  This involved evaluating the data and determining that fundamentally the data would best be stored as 2 tables: one for quote data and the other for author data and having them joined on the authors name.  Thus the code return 2 lists: one with with a lists of lists of the quote data (Quote, Author, Tags) and the other with a list of dictionaries containing the organized Author data (Name, Birthdate, and Bio)\n",
    "### Overview of the code :  After the importing the required libaries and initiating the broswser the two database prepared output lists  are defined. Next the main loop that is the basis of the automation is established that will crawl the whole site to scrape the data.  It begins on the home page and branches in a down then sideways direction.  The first main portion of the code is the main code for crawling the quote data and placing it into a list of lists.  It uses a series of loops to deal with the one big 'gotcha' that exists in the quote data that makes simple scrape code irrelevant because it will not be able to return the data appropriately. The second part of the main code is focused on the sideways  crawl of the author data.  The code opens each of the pages and scrapes the required data and places it an organized dictionary which is then appended the master list for output.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "from flask import Flask, redirect, render_template, jsonify\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import lxml\n",
    "import urllib\n",
    "\n",
    "browser=Browser(\"chrome\", headless=False)\n",
    "\n",
    "total_quotes=[]\n",
    "author_list=[]\n",
    "\n",
    "for r in range(1,11):\n",
    "    url=f'http://quotes.toscrape.com/page/{r}'\n",
    "    response=requests.get(url)\n",
    "    browser.visit(url)\n",
    "    soup=bs(response.text, \"lxml\")\n",
    "\n",
    "    ascrape=soup.find_all('div',{'class':'quote'})\n",
    "    for a in ascrape:\n",
    "        tags_list=[]\n",
    "        text=a.find('span', class_='text').text\n",
    "        author=a.find('small', class_='author').text\n",
    "        tags=a.find('div', class_='tags').find_all('a')\n",
    "        for tag in tags:\n",
    "            tag_text=tag.text.strip()\n",
    "            tags_list.append(tag_text)\n",
    "        one_quote=[text,author,tags_list]\n",
    "        total_quotes.append(one_quote)\n",
    "\n",
    "    about_links=browser.find_by_text(\"(about)\")\n",
    "    href_list=[]\n",
    "    for l in about_links:\n",
    "        ref=l['href']\n",
    "        href_list.append(ref)\n",
    "    for h in href_list:\n",
    "        print(h)\n",
    "        browser.visit(h)\n",
    "        req=requests.get(h)\n",
    "        soup=bs(req.text, \"lxml\")\n",
    "        author_birth=soup.find('span', class_=\"author-born-date\").text\n",
    "        author_desc=soup.find('div', class_=\"author-description\").text\n",
    "        author_name=soup.find('h3', class_=\"author-title\").text\n",
    "        author_dict={'Name':author_name.strip(), 'Birthdate': author_birth.strip(), 'Bio':author_desc.strip()}\n",
    "        author_list.append(author_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}